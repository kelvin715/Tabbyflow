{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Matching for Mixed (Numerical + Categorical) Tabular Data\n",
    "\n",
    "This notebook implements **Flow Matching** for datasets with **both numerical and categorical features**.\n",
    "\n",
    "### Architecture\n",
    "- **Data Processing**: Uses `EFVFMDataset` from `ef-vfm` for data loading and quantile normalization.\n",
    "- **Network**: Simple MLP with sinusoidal time encoding (from `Flow_Matching.ipynb`). Input is `[x_num_t, x_cat_t_onehot, time_enc(t)]`, output is `[pred_num, pred_logits_cat]`.\n",
    "- **Training**: Mixed loss combining numerical and categorical objectives:\n",
    "  - Numerical: MSE between predicted denoised values and true numerical features.\n",
    "  - Categorical: Cross-entropy loss from `ef-vfm` (`_absorbed_closs`).\n",
    "- **Sampling**: ODE integration via `torchdiffeq.odeint` with a velocity field derived from the model's denoised predictions.\n",
    "- **Evaluation**: `TabMetrics` from `ef-vfm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import *\n",
    "from torchdiffeq import odeint\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), 'ef-vfm'))\n",
    "\n",
    "from utils_train import EFVFMDataset\n",
    "from ef_vfm.metrics import TabMetrics\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Mixed Flow Matching\n",
    "\n",
    "Data is represented as $x = [x^{\\text{num}}, x^{\\text{cat}}_{\\text{oh}}]$ where numerical features are continuous\n",
    "and categorical features are one-hot encoded.\n",
    "\n",
    "The OT conditional flow interpolates between noise $x_0 \\sim \\mathcal{N}(0, I)$ and data $x_1$:\n",
    "\n",
    "$$ \\psi_t(x_0) = (1 - (1 - \\sigma_{min})t)\\, x_0 + t\\, x_1 $$\n",
    "\n",
    "The model predicts the **denoised** data $\\hat{x}_1 = f_\\theta(x_t, t)$, split into:\n",
    "- $\\hat{x}_1^{\\text{num}}$: predicted numerical values\n",
    "- $\\hat{x}_1^{\\text{cat}}$: predicted logits per categorical feature\n",
    "\n",
    "The mixed training loss combines both objectives:\n",
    "\n",
    "$$ \\mathcal{L} = \\underbrace{\\| \\hat{x}_1^{\\text{num}} - x_1^{\\text{num}} \\|^2}_{\\text{MSE (numerical)}} + \\underbrace{\\left( -\\sum_j \\log p_\\theta(c_j \\mid x_t, t) \\right)}_{\\text{Cross-Entropy (categorical)}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedFlowMatching:\n",
    "    \"\"\"Flow Matching for mixed numerical + categorical data.\"\"\"\n",
    "\n",
    "    def __init__(self, d_numerical: int, categories: List[int], sig_min: float = 1e-4) -> None:\n",
    "        self.d_numerical = d_numerical\n",
    "        self.categories = categories\n",
    "        self.sig_min = sig_min\n",
    "        self.d_cat_oh = sum(categories)\n",
    "        self.d_input = d_numerical + self.d_cat_oh\n",
    "\n",
    "    def to_one_hot(self, x_cat: torch.Tensor) -> torch.Tensor:\n",
    "        parts = []\n",
    "        for i, k in enumerate(self.categories):\n",
    "            parts.append(F.one_hot(x_cat[:, i].long(), num_classes=k))\n",
    "        return torch.cat(parts, dim=-1).float()\n",
    "\n",
    "    def psi_t(self, x_0: torch.Tensor, x_1: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
    "        return (1 - (1 - self.sig_min) * t) * x_0 + t * x_1\n",
    "\n",
    "    def _absorbed_closs(self, logits: torch.Tensor, x_cat: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Cross-entropy loss over categorical features (from ef-vfm).\"\"\"\n",
    "        cum_sum = 0\n",
    "        losses = torch.zeros(len(self.categories), device=logits.device)\n",
    "        for i, val in enumerate(self.categories):\n",
    "            dist = torch.distributions.Categorical(logits=logits[:, cum_sum:cum_sum + val])\n",
    "            losses[i] = -dist.log_prob(x_cat[:, i].long()).mean()\n",
    "            cum_sum += val\n",
    "        return losses.sum()\n",
    "\n",
    "    def loss(self, model: nn.Module, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Compute mixed training loss.\n",
    "        x: [batch, d_numerical + n_cat_features] where first d_numerical cols are float,\n",
    "           remaining cols are integer category indices.\n",
    "        \"\"\"\n",
    "        b = x.shape[0]\n",
    "        dev = x.device\n",
    "\n",
    "        x_num = x[:, :self.d_numerical].float()\n",
    "        x_cat = x[:, self.d_numerical:]\n",
    "\n",
    "        t = torch.rand(b, device=dev)\n",
    "        t_expand = t[:, None]\n",
    "\n",
    "        # Numerical interpolation\n",
    "        if self.d_numerical > 0:\n",
    "            x_0_num = torch.randn_like(x_num)\n",
    "            x_t_num = self.psi_t(x_0_num, x_num, t_expand)\n",
    "        else:\n",
    "            x_t_num = torch.zeros_like(x_num)\n",
    "\n",
    "        # Categorical interpolation (in one-hot space)\n",
    "        if self.d_cat_oh > 0:\n",
    "            x_1_cat_oh = self.to_one_hot(x_cat)\n",
    "            x_0_cat = torch.randn(b, self.d_cat_oh, device=dev)\n",
    "            x_t_cat = self.psi_t(x_0_cat, x_1_cat_oh, t_expand)\n",
    "        else:\n",
    "            x_t_cat = torch.zeros_like(x_cat)\n",
    "\n",
    "        # Concatenate and predict\n",
    "        x_t = torch.cat([x_t_num, x_t_cat], dim=1)\n",
    "        pred = model(t, x_t)\n",
    "\n",
    "        pred_num = pred[:, :self.d_numerical]\n",
    "        pred_logits = pred[:, self.d_numerical:]\n",
    "\n",
    "        # Numerical loss: MSE on denoised prediction\n",
    "        if self.d_numerical > 0:\n",
    "            num_loss = F.mse_loss(pred_num, x_num)\n",
    "        else:\n",
    "            num_loss = torch.tensor(0.0, device=dev)\n",
    "\n",
    "        # Categorical loss: cross-entropy\n",
    "        if self.d_cat_oh > 0:\n",
    "            cat_loss = self._absorbed_closs(pred_logits, x_cat)\n",
    "        else:\n",
    "            cat_loss = torch.tensor(0.0, device=dev)\n",
    "\n",
    "        return num_loss, cat_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural Network (MLP with Sinusoidal Time Encoding)\n",
    "\n",
    "A simple MLP that takes `[x_num_t, x_cat_t_onehot, time_encoding(t)]` and outputs `[pred_num, pred_logits_cat]`.\n",
    "The output dimension equals `d_numerical + sum(categories)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \"\"\"MLP with sinusoidal time encoding (from Flow_Matching.ipynb).\"\"\"\n",
    "\n",
    "    def __init__(self, in_dim: int, out_dim: int, h_dims: List[int], n_frequencies: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        ins = [in_dim + 2 * n_frequencies] + h_dims\n",
    "        outs = h_dims + [out_dim]\n",
    "        self.n_frequencies = n_frequencies\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(in_d, out_d), nn.LeakyReLU())\n",
    "            for in_d, out_d in zip(ins, outs)\n",
    "        ])\n",
    "        self.top = nn.Sequential(nn.Linear(out_dim, out_dim))\n",
    "\n",
    "    def time_encoder(self, t: torch.Tensor) -> torch.Tensor:\n",
    "        freq = 2 * torch.arange(self.n_frequencies, device=t.device) * torch.pi\n",
    "        t = freq * t[..., None]\n",
    "        return torch.cat((t.cos(), t.sin()), dim=-1)\n",
    "\n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        t = self.time_encoder(t)\n",
    "        x = torch.cat((x, t), dim=-1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.top(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mixed Velocity Wrapper (for ODE Sampling)\n",
    "\n",
    "During sampling, the model predicts denoised data, which is converted to a velocity field:\n",
    "\n",
    "$$ v_t(x_t) = \\frac{\\hat{x}_1 - (1 - \\sigma_{min}) \\cdot x_t}{1 - (1 - \\sigma_{min}) \\cdot t} $$\n",
    "\n",
    "For numerical features, $\\hat{x}_1^{\\text{num}}$ is used directly.\n",
    "For categorical features, $\\hat{x}_1^{\\text{cat}} = \\text{softmax}(\\text{logits})$ converts logits to probability simplex.\n",
    "\n",
    "After ODE integration:\n",
    "- Numerical outputs are used directly as continuous values.\n",
    "- Categorical outputs are discretized via `argmax` per feature segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedVelocity(nn.Module):\n",
    "    \"\"\"Converts model denoised predictions to velocity for ODE sampling.\"\"\"\n",
    "\n",
    "    def __init__(self, model: nn.Module, d_numerical: int, categories: List[int], sig_min: float = 1e-4):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.d_numerical = d_numerical\n",
    "        self.categories = categories\n",
    "        self.sig_min = sig_min\n",
    "\n",
    "    def forward(self, t: torch.Tensor, x: torch.Tensor) -> torch.Tensor:\n",
    "        if t.dim() == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        t_batch = t.expand(x.shape[0])\n",
    "\n",
    "        pred = self.model(t_batch, x)\n",
    "        denom = 1 - (1 - self.sig_min) * t_batch.unsqueeze(1)\n",
    "\n",
    "        # Numerical velocity: pred_num is already the denoised prediction\n",
    "        if self.d_numerical > 0:\n",
    "            pred_num = pred[:, :self.d_numerical]\n",
    "            x_num = x[:, :self.d_numerical]\n",
    "            v_num = (pred_num - (1 - self.sig_min) * x_num) / denom\n",
    "        else:\n",
    "            v_num = torch.zeros_like(x[:, :self.d_numerical])\n",
    "            \n",
    "        # Categorical velocity: softmax(logits) as denoised prediction\n",
    "        if len(self.categories) > 0:\n",
    "            pred_logits = pred[:, self.d_numerical:]\n",
    "            x_cat = x[:, self.d_numerical:]\n",
    "\n",
    "            # my version\n",
    "            v_cat_parts = []\n",
    "            logit_idx = 0\n",
    "            oh_idx = 0\n",
    "            for k in self.categories:\n",
    "                probs_k = F.softmax(pred_logits[:, logit_idx:logit_idx + k], dim=-1)\n",
    "                x_k = x_cat[:, oh_idx:oh_idx + k]\n",
    "                v_k = (probs_k - (1 - self.sig_min) * x_k) / denom\n",
    "                v_cat_parts.append(v_k)\n",
    "                logit_idx += k\n",
    "                oh_idx += k\n",
    "            v_cat = torch.cat(v_cat_parts, dim=1)\n",
    "            \n",
    "            # Guzman's version, the performance is bad\n",
    "            # v_cat = (pred_logits - (1 - self.sig_min) * x_cat) / denom\n",
    "            \n",
    "        else:\n",
    "            v_cat = torch.zeros_like(x[:, self.d_numerical:])\n",
    "\n",
    "        return torch.cat([v_num, v_cat], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Loading\n",
    "\n",
    "Load a mixed dataset using `EFVFMDataset`. The dataset should have both `d_numerical > 0` and `len(categories) > 0`.\n",
    "\n",
    "Available mixed datasets: `adult`, `default`, `shoppers`, `beijing`, `news`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ Configuration ============\n",
    "dataname = 'news_numerical_only'\n",
    "data_dir = f'ef-vfm-dev/data/{dataname}'\n",
    "info_path = f'ef-vfm-dev/data/{dataname}/info.json'\n",
    "\n",
    "with open(info_path, 'r') as f:\n",
    "    info = json.load(f)\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2048\n",
    "n_epochs = 100\n",
    "lr = 1e-3\n",
    "sigma_min = 1e-4\n",
    "h_dims = [512] * 5\n",
    "n_frequencies = 10\n",
    "\n",
    "# Load Data\n",
    "train_data = EFVFMDataset(\n",
    "    dataname, data_dir, info, isTrain=True,\n",
    "    dequant_dist='none', int_dequant_factor=0.0\n",
    ")\n",
    "\n",
    "d_numerical = train_data.d_numerical\n",
    "categories = train_data.categories.tolist()\n",
    "\n",
    "# assert d_numerical > 0, f\"Expected numerical features, but d_numerical={d_numerical}\"\n",
    "# assert len(categories) > 0, f\"Expected categorical features, but categories is empty\"\n",
    "\n",
    "d_cat_oh = sum(categories)\n",
    "d_total = d_numerical + d_cat_oh\n",
    "\n",
    "print(f\"Dataset: {dataname}\")\n",
    "print(f\"d_numerical: {d_numerical}\")\n",
    "print(f\"categories (classes per feature): {categories}\")\n",
    "print(f\"d_cat_oh (total one-hot dim): {d_cat_oh}\")\n",
    "print(f\"d_total (model input/output dim): {d_total}\")\n",
    "print(f\"Training data shape: {train_data.X.shape}\")\n",
    "\n",
    "dataset = train_data.X.to(device)\n",
    "tensor_dataset = TensorDataset(dataset)\n",
    "dataloader = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Number of batches: {len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training\n",
    "\n",
    "Train the MLP with the mixed loss: MSE for numerical denoising + cross-entropy for categorical denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dim = d_total\n",
    "out_dim = d_total\n",
    "\n",
    "mixed_fm = MixedFlowMatching(d_numerical, categories, sig_min=sigma_min)\n",
    "net = Net(in_dim, out_dim, h_dims, n_frequencies).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in net.parameters()):,}\")\n",
    "print(f\"Training for {n_epochs} epochs...\")\n",
    "\n",
    "losses_num = []\n",
    "losses_cat = []\n",
    "losses_total = []\n",
    "net.train()\n",
    "\n",
    "bar = tqdm(range(n_epochs), ncols=100)\n",
    "for epoch in bar:\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        num_loss, cat_loss = mixed_fm.loss(net, x)\n",
    "        loss = num_loss + cat_loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses_num.append(num_loss.item())\n",
    "        losses_cat.append(cat_loss.item())\n",
    "        losses_total.append(loss.item())\n",
    "    bar.set_postfix(total=loss.item(), num=num_loss.item(), cat=cat_loss.item())\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "axes[0].plot(losses_total)\n",
    "axes[0].set_title(\"Total Loss\")\n",
    "axes[0].set_yscale('log')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[1].plot(losses_num)\n",
    "axes[1].set_title(\"Numerical Loss (MSE)\")\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[2].plot(losses_cat)\n",
    "axes[2].set_title(\"Categorical Loss (CE)\")\n",
    "axes[2].set_yscale('log')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "for ax in axes:\n",
    "    ax.set_xlabel(\"Iteration\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sampling\n",
    "\n",
    "Generate synthetic data by:\n",
    "1. Sampling $x_0 \\sim \\mathcal{N}(0, I)$ in $\\mathbb{R}^{d_{\\text{num}} + \\sum K_j}$\n",
    "2. ODE integration from $t=0$ to $t=1$ using the mixed velocity field\n",
    "3. Numerical features are taken directly; categorical features are discretized via `argmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_num_cat_target(syn_data, info, num_inverse, int_inverse, cat_inverse):\n",
    "    task_type = info['task_type']\n",
    "    num_col_idx = info['num_col_idx']\n",
    "    cat_col_idx = info['cat_col_idx']\n",
    "    target_col_idx = info['target_col_idx']\n",
    "\n",
    "    if hasattr(num_inverse, 'n_features_in_'):\n",
    "        n_num_feat = num_inverse.n_features_in_\n",
    "    else:\n",
    "        n_num_feat = len(num_col_idx)\n",
    "        if task_type == 'regression':\n",
    "            n_num_feat += len(target_col_idx)\n",
    "\n",
    "    if hasattr(cat_inverse, 'n_features_in_'):\n",
    "        n_cat_feat = cat_inverse.n_features_in_\n",
    "    else:\n",
    "        n_cat_feat = len(cat_col_idx)\n",
    "        if task_type != 'regression':\n",
    "            n_cat_feat += len(target_col_idx)\n",
    "\n",
    "    syn_num = syn_data[:, :n_num_feat]\n",
    "    syn_cat = syn_data[:, n_num_feat:] if n_cat_feat > 0 else syn_data[:, n_num_feat:]\n",
    "\n",
    "    syn_num = num_inverse(syn_num).astype(np.float32)\n",
    "    syn_num = int_inverse(syn_num).astype(np.float32)\n",
    "    if n_cat_feat > 0:\n",
    "        syn_cat = cat_inverse(syn_cat)\n",
    "    else:\n",
    "        syn_cat = np.empty((syn_data.shape[0], 0))\n",
    "\n",
    "    if info['task_type'] == 'regression':\n",
    "        syn_target = syn_num[:, :len(target_col_idx)]\n",
    "        syn_num = syn_num[:, len(target_col_idx):]\n",
    "    else:\n",
    "        syn_target = syn_cat[:, :len(target_col_idx)]\n",
    "        syn_cat = syn_cat[:, len(target_col_idx):]\n",
    "\n",
    "    return syn_num, syn_cat, syn_target\n",
    "\n",
    "\n",
    "def recover_data(syn_num, syn_cat, syn_target, info):\n",
    "    num_col_idx = info['num_col_idx']\n",
    "    cat_col_idx = info['cat_col_idx']\n",
    "    target_col_idx = info['target_col_idx']\n",
    "    idx_mapping = info['idx_mapping']\n",
    "    idx_mapping = {int(key): value for key, value in idx_mapping.items()}\n",
    "\n",
    "    syn_df = pd.DataFrame()\n",
    "    if info['task_type'] == 'regression':\n",
    "        for i in range(len(num_col_idx) + len(cat_col_idx) + len(target_col_idx)):\n",
    "            if i in set(num_col_idx):\n",
    "                syn_df[i] = syn_num[:, idx_mapping[i]]\n",
    "            elif i in set(cat_col_idx):\n",
    "                syn_df[i] = syn_cat[:, idx_mapping[i] - len(num_col_idx)]\n",
    "            else:\n",
    "                syn_df[i] = syn_target[:, idx_mapping[i] - len(num_col_idx) - len(cat_col_idx)]\n",
    "    else:\n",
    "        for i in range(len(num_col_idx) + len(cat_col_idx) + len(target_col_idx)):\n",
    "            if i in set(num_col_idx):\n",
    "                syn_df[i] = syn_num[:, idx_mapping[i]]\n",
    "            elif i in set(cat_col_idx):\n",
    "                syn_df[i] = syn_cat[:, idx_mapping[i] - len(num_col_idx)]\n",
    "            else:\n",
    "                syn_df[i] = syn_target[:, idx_mapping[i] - len(num_col_idx) - len(cat_col_idx)]\n",
    "    return syn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(train_data)\n",
    "print(f\"Generating {num_samples} samples...\")\n",
    "\n",
    "net.eval()\n",
    "velocity = MixedVelocity(net, d_numerical, categories, sig_min=sigma_min)\n",
    "\n",
    "sample_batch_size = 4096\n",
    "all_samples = []\n",
    "num_generated = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    while num_generated < num_samples:\n",
    "        cur_batch = min(sample_batch_size, num_samples - num_generated)\n",
    "        x_0 = torch.randn(cur_batch, d_total, device=device)\n",
    "        t_span = torch.tensor([0.0, 0.999], device=device)\n",
    "        trajectory = odeint(velocity, x_0, t_span, method='dopri5', rtol=1e-5, atol=1e-5)\n",
    "        out = trajectory[1]\n",
    "\n",
    "        # Numerical part: take directly\n",
    "        sample_num = out[:, :d_numerical]\n",
    "\n",
    "        # Categorical part: argmax per feature\n",
    "        cat_out = out[:, d_numerical:]\n",
    "        sample_cat = torch.zeros(cur_batch, len(categories), device=device)\n",
    "        oh_idx = 0\n",
    "        for i, k in enumerate(categories):\n",
    "            sample_cat[:, i] = torch.argmax(cat_out[:, oh_idx:oh_idx + k], dim=1).float()\n",
    "            oh_idx += k\n",
    "\n",
    "        sample = torch.cat([sample_num, sample_cat], dim=1)\n",
    "\n",
    "        mask_nan = torch.any(sample.isnan(), dim=1)\n",
    "        sample = sample[~mask_nan]\n",
    "        all_samples.append(sample.cpu())\n",
    "        num_generated += sample.shape[0]\n",
    "        print(f\"  Generated {num_generated}/{num_samples} samples\")\n",
    "\n",
    "syn_tensor = torch.cat(all_samples, dim=0)[:num_samples]\n",
    "syn_data_np = syn_tensor.numpy()\n",
    "print(f\"Generated data shape: {syn_data_np.shape}\")\n",
    "\n",
    "num_inverse = train_data.num_inverse\n",
    "int_inverse = train_data.int_inverse\n",
    "cat_inverse = train_data.cat_inverse\n",
    "\n",
    "syn_num, syn_cat, syn_target = split_num_cat_target(\n",
    "    syn_data_np, info, num_inverse, int_inverse, cat_inverse\n",
    ")\n",
    "syn_df = recover_data(syn_num, syn_cat, syn_target, info)\n",
    "\n",
    "idx_name_mapping = info['idx_name_mapping']\n",
    "idx_name_mapping = {int(key): value for key, value in idx_name_mapping.items()}\n",
    "syn_df.rename(columns=idx_name_mapping, inplace=True)\n",
    "\n",
    "print(f\"\\nSampled Data Head:\")\n",
    "print(syn_df.head())\n",
    "print(f\"\\nGenerated {len(syn_df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n",
    "\n",
    "Using `ef-vfm`'s `TabMetrics` to evaluate the generated samples with density, MLE, and C2ST metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_path = f'ef-vfm-dev/synthetic/{dataname}/real.csv'\n",
    "test_data_path = f'ef-vfm-dev/synthetic/{dataname}/test.csv'\n",
    "val_data_path = f'ef-vfm-dev/synthetic/{dataname}/val.csv'\n",
    "\n",
    "if not os.path.exists(val_data_path):\n",
    "    print(f\"{dataname} does not have a validation set. MLE evaluation will split from training set.\")\n",
    "    val_data_path = None\n",
    "\n",
    "is_dcr = 'dcr' in dataname\n",
    "if is_dcr:\n",
    "    metric_list = [\"dcr\"]\n",
    "else:\n",
    "    metric_list = [\"density\", \"mle\", \"c2st\"]\n",
    "\n",
    "print(f\"Initializing TabMetrics with metrics: {metric_list}\")\n",
    "metrics = TabMetrics(\n",
    "    real_data_path=real_data_path,\n",
    "    test_data_path=test_data_path,\n",
    "    val_data_path=val_data_path,\n",
    "    info=info,\n",
    "    device=device,\n",
    "    metric_list=metric_list\n",
    ")\n",
    "\n",
    "print(f\"Real data size: {metrics.real_data_size}\")\n",
    "print(f\"Generated data size: {len(syn_df)}\")\n",
    "\n",
    "if len(syn_df) != metrics.real_data_size:\n",
    "    print(f\"\\nRegenerating samples to match real data size ({metrics.real_data_size})...\")\n",
    "    net.eval()\n",
    "    target_n = metrics.real_data_size\n",
    "    all_samples = []\n",
    "    num_generated = 0\n",
    "    with torch.no_grad():\n",
    "        while num_generated < target_n:\n",
    "            cur_batch = min(sample_batch_size, target_n - num_generated)\n",
    "            x_0 = torch.randn(cur_batch, d_total, device=device)\n",
    "            t_span = torch.tensor([0.0, 0.999], device=device)\n",
    "            trajectory = odeint(velocity, x_0, t_span, method='dopri5', rtol=1e-5, atol=1e-5)\n",
    "            out = trajectory[1]\n",
    "            sample_num = out[:, :d_numerical]\n",
    "            cat_out = out[:, d_numerical:]\n",
    "            sample_cat = torch.zeros(cur_batch, len(categories), device=device)\n",
    "            oh_idx = 0\n",
    "            for i, k in enumerate(categories):\n",
    "                sample_cat[:, i] = torch.argmax(cat_out[:, oh_idx:oh_idx + k], dim=1).float()\n",
    "                oh_idx += k\n",
    "            sample = torch.cat([sample_num, sample_cat], dim=1)\n",
    "            mask_nan = torch.any(sample.isnan(), dim=1)\n",
    "            sample = sample[~mask_nan]\n",
    "            all_samples.append(sample.cpu())\n",
    "            num_generated += sample.shape[0]\n",
    "    syn_tensor = torch.cat(all_samples, dim=0)[:target_n]\n",
    "    syn_data_np = syn_tensor.numpy()\n",
    "    syn_num, syn_cat, syn_target = split_num_cat_target(\n",
    "        syn_data_np, info, num_inverse, int_inverse, cat_inverse\n",
    "    )\n",
    "    syn_df = recover_data(syn_num, syn_cat, syn_target, info)\n",
    "    syn_df.rename(columns=idx_name_mapping, inplace=True)\n",
    "    print(f\"Regenerated {len(syn_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Starting Evaluation...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "temp_csv = tempfile.NamedTemporaryFile(mode='w', suffix='.csv', delete=False)\n",
    "syn_df.to_csv(temp_csv.name, index=False)\n",
    "temp_csv.close()\n",
    "\n",
    "syn_df_for_eval = pd.read_csv(temp_csv.name)\n",
    "syn_df_for_eval.columns = range(len(syn_df_for_eval.columns))\n",
    "\n",
    "out_metrics, extras = metrics.evaluate(syn_df_for_eval)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in out_metrics.items():\n",
    "    if isinstance(value, (int, float)):\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "os.unlink(temp_csv.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "results_dir = f\"results/{dataname}/fm_mixed_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "metrics_path = os.path.join(results_dir, \"metrics.json\")\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(out_metrics, f, indent=4, default=str)\n",
    "print(f\"Metrics saved to: {metrics_path}\")\n",
    "\n",
    "syn_data_path = os.path.join(results_dir, \"synthetic_samples.csv\")\n",
    "syn_df.to_csv(syn_data_path, index=False)\n",
    "print(f\"Synthetic data saved to: {syn_data_path}\")\n",
    "\n",
    "if extras:\n",
    "    for name, extra in extras.items():\n",
    "        if isinstance(extra, pd.DataFrame):\n",
    "            extra_path = os.path.join(results_dir, f\"{name}.csv\")\n",
    "            extra.to_csv(extra_path, index=False)\n",
    "            print(f\"Extra {name} saved to: {extra_path}\")\n",
    "        elif isinstance(extra, dict):\n",
    "            extra_path = os.path.join(results_dir, f\"{name}.json\")\n",
    "            with open(extra_path, 'w') as f:\n",
    "                json.dump(extra, f, indent=4, default=str)\n",
    "            print(f\"Extra {name} saved to: {extra_path}\")\n",
    "\n",
    "print(f\"\\nAll results saved to: {results_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
